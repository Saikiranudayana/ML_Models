# Machine Learning Models ðŸ“˜

This repository contains implementations of various **Machine Learning algorithms**, categorized into **Supervised Learning** and **Unsupervised Learning**.

---

## ðŸ”¹ Supervised Learning
Supervised learning uses **labeled data** (input â†’ output pairs). The model learns from training data and predicts outcomes for new data.  
Examples include **classification** (predicting categories) and **regression** (predicting continuous values).

| Algorithm | Description | Link |
|-----------|-------------|------|
| Decision Tree | Splits data into branches to make predictions | [Decision Tree](./Decision_Tree) |
| Random Forest | Ensemble of decision trees for better accuracy | [Random Forest](./Random_Forest) |
| Gradient Boost | Builds models sequentially, focusing on errors | [Gradient_Boost](./Gradient_Boost) |
| ADA Boost | Adaptive boosting method to improve weak learners | [ADA_Boost](./ADA_Boost) |
| XGBoost | Optimized boosting algorithm for high performance | [XgBoost](./XgBoost) |
| Logistic Regression | Classification using a sigmoid function | [Logistic_Regression](./Logistic_Regression) |
| Naive Bayes Classifier | Probabilistic model based on Bayesâ€™ theorem | [Naive_Bayes_Classifier](./Naive_Bayes_Classifier) |
| K-Nearest Neighbour (KNN) | Predicts label based on closest neighbors | [K_Nearest_Neighbour](./K_Nearest_Neighbour) |
| Support Vector Machine (SVM) | Finds optimal hyperplane for classification | [SVM](./SVM) |
| Simple Linear Regression | Fits a straight line to predict values | [Simple_Linear_Regression](./Simple_Linear_Regression) |
| Multiple Linear Regression | Uses multiple features for regression | [Multiple_Linear_Regression](./Multiple_Linear_Regression) |
| Polynomial Regression | Extends linear regression with polynomial terms | [Polynomial_Regression](./Polynomial_Regression) |
| Ridge & Lasso Regression | Regularized regression to prevent overfitting | [Ridge_Lasso_Regression](./Ridge_Lasso_Regression) |
| Elastic Net Regression | Combines Ridge and Lasso penalties | [Elastic_Net_Regression](./Elastic_Net_Regression) |

---

## ðŸ”¹ Unsupervised Learning
Unsupervised learning works on **unlabeled data**. It finds hidden patterns, groupings, or structures without predefined outputs.  
Examples include **clustering** and **dimensionality reduction**.

| Algorithm | Description | Link |
|-----------|-------------|------|
| K-Means Clustering | Groups data into K clusters based on similarity | [Kmeans_Clustering](./Kmeans_Clustering) |
| Hierarchical Clustering | Builds a tree of clusters (dendrogram) | [Hierarchical_Clustering](./Hierarchical_Clustering) |
| DBSCAN Clustering | Groups data based on density, handles noise | [DBSCAN_Clustering](./DBSCAN_Clustering) |
| Principal Component Analysis (PCA) | Reduces dimensionality while preserving variance | [Principal_Component_Analysis](./Principal_Component_Analysis) |

---

## ðŸš€ How to Use
1. Navigate to the desired model folder.  
2. Each folder contains:  
   - `README.md` â†’ Explanation of the model  
   - Implementation code  
   - Example dataset (if applicable)  

---

